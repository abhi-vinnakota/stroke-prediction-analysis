{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abhi Vinnakota and Gnandeep Chintala\n",
    "#Quarter 3 Project\n",
    "#3-24-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df = pd.read_csv(\".\\healthcare-dataset-stroke-data.csv\") #reading the data file\n",
    "stroke_df['smoking_status'].replace('Unknown', np.nan, inplace=True) #replacing unknown values in smoking status column with NaN\n",
    "stroke_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stroke_df['stroke'].value_counts(), '\\n')  #counting the amount of stroke values vs missing values or NaN\n",
    "print(stroke_df['stroke'].value_counts(normalize=True)) #percent of stroke values vs missing values\n",
    "print(\"Samples:\", stroke_df.shape[0]) #number of total samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Therefore, there are 5110 total stroke data samples, with only about 5% or 249 being positive stroke results. These rows are valuable, and it would be detrimental to remove these rows when cleaning the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the smoking_status feature has 1544 missing values, which is more than 20 percent of the total samples, it would not be beneficial to remove just those missing rows. Instead, removing the entire smoking_status feature may result in a better analysis. Similarly, removing the bmi column entirely would also be useful. It would not help to remove all of these missing value rows, because they may contain many of the positive stroke results, which would negatively impact the classification. For this reason, we decided to drop both smoking_status and bmi, even though these are factors that could affect a stroke. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stroke_df.drop(['id', 'ever_married', 'bmi', 'smoking_status', 'work_type', 'gender', 'Residence_type', 'stroke'], axis = 1)\n",
    "y = stroke_df['stroke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We chose these features for classification after considering which columns would be most useful. After observing that the bmi and smoking_status columns had many missing values, we decided to drop both of them because we did not want to sacrifice samples that could be useful during the classification. The id column was dropped because it was simply not needed. The rest of the columns were dropped because they did not seem very relevant to the stroke data. For example, even though one's work type could influence their chance of getting a stroke, it is an external factor that isn't a measure of someone's health. Therefore, we chose to exclude such features and instead focus on features such as age that have the most impact on the chance of a stroke. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0) #20 percent train test split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Stroke Occurrence with Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier = knn_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(classifier, X_test, y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The KNeighbors Classifier correctly predicted 94% of the samples with a .94 accuracy value. This means that it correctly predicted 94 percent of the time that a patient would not get a stroke. Because the recall is is .99, there is a high number of true positives compared to false negatives. The precision at .95 is also very high, meaning there were a more true positives compared to false positives. A confusion matrix is also plotted above to visualize the overwhelming amount of true positives returned by the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(random_state = 42, n_estimators = 500)\n",
    "classifier = rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(classifier, X_test, y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Random Forest classifier had similar results to the KNeighbors classifier with a .98 recall score and .95 precision score. It ran almost as well as the KNeighbors but just a little bit worse. For this reason, we concluded that the KNeighbors classifier best predicted whether the patient had a stroke or not. As shown by the confusion matrix, the Random Forest classifier also did well at getting true positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Average Glucose Level with Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stroke_df.drop(['id', 'ever_married', 'bmi', 'smoking_status', 'work_type', 'gender', 'Residence_type', 'avg_glucose_level'], axis = 1)\n",
    "y = stroke_df['avg_glucose_level']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "#scaling the training set\n",
    "X_train = sc.fit_transform(X_train)\n",
    "#scaling the test set\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
    "regressor = knn_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "from sklearn import metrics \n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_reg = RandomForestRegressor(random_state=42, n_estimators=500)\n",
    "regressor = rf_reg.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_reg = svm.SVR()\n",
    "\n",
    "regressor = svm_reg.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "from sklearn import metrics \n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After running the three regression methods, we found that SVM regression returned the lowest Mean Absolute Error, meaning that it best predicted the average glucose level of a patient based on stroke occurence, age, and history of hypertension and heart disease. With a mean absolute error of about 29.5, the SVM regressor was able to predict the average glucose level of a patient with an error of 29.5 mmol/L. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization of Stroke Patient Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clustering to find Insights/Correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age', 'avg_glucose_level', 'bmi'] #these numerical columns are health factors that can affect stroke likelihood\n",
    "sns.pairplot(stroke_df[['stroke'] + features].sort_values('stroke'), hue='stroke', height=2) #creating a pairplot for the stroke column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pairplot returned various plots which relate occurrence of a stroke to either BMI, age, or average glucose level. We concluded from the pairplot that age and glucose level have a large impact on stroke occurrence while BMI has the least impact. This is also supported by the clustering done below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stroke_df.drop(['id', 'ever_married', 'bmi', 'smoking_status', 'work_type', 'gender', 'Residence_type', 'stroke'], axis = 1)\n",
    "y = stroke_df['stroke']\n",
    "\n",
    "#dividing data into features and labels\n",
    "features = X.filter(['age','avg_glucose_level'],axis = 1)\n",
    "labels = y\n",
    "#training KMeans model\n",
    "features = features.values\n",
    "km_model = KMeans(n_clusters = 2) #creating two clusters, one for positive stroke occurrence and one for negative\n",
    "km_model.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Age')\n",
    "plt.ylabel('Glucose Level')\n",
    "plt.title('Relationship between Age, Glucose Levels, and the Event of Having a Stroke')\n",
    "\n",
    "plt.scatter(features[:,0], features[:,1], c = km_model.labels_, cmap = 'rainbow')\n",
    "\n",
    "plt.scatter(km_model.cluster_centers_[:,0], km_model.cluster_centers_[:,1], s = 100, c = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first data clustering we ran was to find the relationship between age, average glucose level, and the occurence of a stroke. The points in red represent a patient who had a stroke, and the point in blue represent a patient who did not have a stroke. As you can see on the plot above, there seems to be a divide between the red and blue points around the 150 glucose level mark. This must mean there is a threshold where patients with glucose levels above 150 can have a stroke. We also noticed that the density of the red points increases as the age gets higher, which supports the conclusion that older people are more at risk for a stroke. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df = stroke_df.dropna() #dropping all NaN values from BMI column\n",
    "stroke_df = stroke_df.reset_index(drop=True)\n",
    "\n",
    "stroke_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stroke_df.drop(['id', 'ever_married', 'smoking_status', 'work_type', 'gender', 'Residence_type', 'stroke'], axis = 1)\n",
    "y = stroke_df['stroke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing data into features and labels\n",
    "features = X.filter(['avg_glucose_level','bmi'],axis = 1)\n",
    "labels = y\n",
    "#training KMeans model\n",
    "features = features.values\n",
    "km_model = KMeans(n_clusters = 2)\n",
    "km_model.fit(features)\n",
    "#print the data points\n",
    "#plt.scatter(ages,glucose_levels, c = 'red')\n",
    "\n",
    "plt.xlabel('Glucose Level')\n",
    "plt.ylabel('BMI')\n",
    "plt.title('Relationship between BMI, Glucose Levels, and the Event of Having a Stroke')\n",
    "#print the data points\n",
    "plt.scatter(features[:,0], features[:,1], c = km_model.labels_, cmap = 'rainbow')\n",
    "\n",
    "#print the centroids\n",
    "plt.scatter(km_model.cluster_centers_[:,0], km_model.cluster_centers_[:,1], s=100, c = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After plotting clusters relating BMI to glucose level, we found a similar threshold at 150 mmol/L where patients above this level had a stroke. We also observed that the BMI didn't seem to have a large impact on the clustering, however it is interesting to note that the red cluster is most dense between 20 and 50 BMI. This either means that most of the patients were at this BMI level, or that stroke happen more frequenly at that level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall, we were happy with the results. With correlation we were able to predict the occurence of a stroke, with regression we were able to predict the glucose level based on stroke occurence and other factors, and finally with clustering we were able to visualize and prove which factors have the most impact in the likelihood of a stroke. Our conclusions are discussed more in depth in the write up for the project. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
